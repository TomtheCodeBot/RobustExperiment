{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-26 16:53:24.122758: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from model.BERTNoiseDefend import BertForSequenceClassification\n",
    "from model.RoBERTaNoiseDefend import RobertaForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from textattack.models.wrappers import (\n",
    "    ModelWrapper,\n",
    "    PyTorchModelWrapper,\n",
    "    SklearnModelWrapper,\n",
    "    HuggingFaceModelWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"model/weights/bert-base-uncased-imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"model/weights/bert-base-uncased-imdb\", use_fast=True\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification(config)\n",
    "state = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"model/weights/bert-base-uncased-imdb\"\n",
    ")\n",
    "\n",
    "#config = AutoConfig.from_pretrained(\"textattack/bert-base-uncased-ag-news\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\n",
    "#    \"textattack/bert-base-uncased-ag-news\", use_fast=True\n",
    "#)\n",
    "#\n",
    "#model = BertForSequenceClassification(config)\n",
    "#state = AutoModelForSequenceClassification.from_pretrained(\n",
    "#    \"textattack/bert-base-uncased-ag-news\"\n",
    "#)\n",
    "model.load_state_dict(state.state_dict())\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "token_length = 256\n",
    "tokenizer.model_max_length=token_length\n",
    "tokens = tokenizer([\"This film is terrible. You don't really need to read this review further. If you are planning on watching it, suffice to say - don't (unless you are studying how not to make a good movie).<br /><br />The acting is horrendous... serious amateur hour. Throughout the movie I thought that it was interesting that they found someone who speaks and looks like Michael Madsen, only to find out that it is actually him! A new low even for him!!<br /><br />The plot is terrible. People who claim that it is original or good have probably never seen a decent movie before. Even by the standard of Hollywood action flicks, this is a terrible movie.<br /><br />Don't watch it!!! Go for a jog instead - at least you won't feel like killing yourself.\",\"This film is terrible. You don't really need to read this review further. If you are planning on watching it, suffice to say - don't (unless you are studying how not to make a good movie).<br /><br />The acting is horrendous... serious amateur hour. Throughout the movie I thought that it was interesting that they found someone who speaks and looks like Michael Madsen, only to find out that it is actually him! A new low even for him!!<br /><br />The plot is terrible. People who claim that it is original or good have probably never seen a decent movie before. Even by the standard of Hollywood action flicks, this is a terrible movie.<br /><br />Don't watch it!!! Go for a jog instead - at least you won't feel like killing yourself.\"],add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=token_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "output = model(tokens[\"input_ids\"],tokens[\"token_type_ids\"],tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n",
      "torch.Size([2, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "for layers in model.bert.encoder.layer:\n",
    "    print(layers.attention_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [21:52<00:00, 15.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from utils.dataloader import load_train_test_imdb_data\n",
    "import numpy as np\n",
    "import datasets\n",
    "num_repetitions = 3\n",
    "batch=300\n",
    "y_pred_BERT = []\n",
    "train_data, test_data = load_train_test_imdb_data(\n",
    "        \"data/aclImdb\"\n",
    "    )\n",
    "token_length = 256\n",
    "\n",
    "#sst2_dataset = datasets.load_dataset(\"ag_news\")\n",
    "#train_data = sst2_dataset[\"train\"]\n",
    "#test_data = sst2_dataset[\"test\"]\n",
    "\n",
    "test_labels = np.array(test_data[\"label\"])\n",
    "bert_input = list(train_data[\"text\"])\n",
    "device = \"cuda\"\n",
    "BERT = HuggingFaceModelWrapper(model,tokenizer)\n",
    "std_features = [None] * len(model.bert.encoder.layer)\n",
    "mean_features = [None] * len(model.bert.encoder.layer)\n",
    "for i in tqdm(range(0,len(bert_input)//batch)):\n",
    "    BERT(bert_input[i*batch:(i+1)*batch])\n",
    "    for k in range(0,len(model.bert.encoder.layer)):\n",
    "        if std_features[k] is None:\n",
    "            std_features[k] = torch.std(model.bert.encoder.layer[k].attention_output.cpu(), dim=1)\n",
    "            mean_features[k] = torch.mean(model.bert.encoder.layer[k].attention_output.cpu(), dim=1)\n",
    "        else:\n",
    "            std_features[k] = torch.cat((std_features[k], torch.std(model.bert.encoder.layer[k].attention_output.cpu(), dim=1)), 0)\n",
    "            mean_features[k] = torch.cat((mean_features[k], torch.mean(model.bert.encoder.layer[k].attention_output.cpu(), dim=1)), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7077, 0.6871, 0.6129, 0.4894, 0.7485, 0.7143, 0.5300, 0.6213, 0.6867,\n",
       "        0.6139, 0.4979, 0.5418, 0.6302, 0.7276, 0.6055, 0.5985, 0.5686, 0.5637,\n",
       "        0.6699, 0.5814, 0.6581, 0.4979, 0.6608, 0.8249, 0.5756, 0.6723, 0.6272,\n",
       "        0.6377, 0.5875, 0.5100, 0.5784, 0.7499, 0.7780, 0.6422, 0.6850, 0.5305,\n",
       "        0.7380, 0.6117, 0.6848, 0.7503, 0.5564, 0.5842, 0.6640, 0.7983, 0.6015,\n",
       "        0.5398, 0.7789, 0.6039, 0.5873, 0.6397, 0.6443, 0.5636, 0.7471, 0.5026,\n",
       "        0.5622, 0.8786, 0.6033, 0.5454, 0.5530, 0.5923, 0.6161, 0.5145, 0.5105,\n",
       "        0.5772, 0.6860, 0.7123, 0.5608, 0.5917, 1.1347, 0.7233, 0.5703, 0.5986,\n",
       "        0.6857, 0.5697, 0.6539, 0.6227, 0.6788, 0.5577, 0.5776, 0.6092, 0.6672,\n",
       "        0.5477, 0.7048, 0.5766, 0.6855, 0.5324, 0.6712, 0.6928, 0.7110, 0.7450,\n",
       "        0.9230, 0.7267, 0.6039, 0.6843, 0.5374, 0.5490, 0.5205, 0.5011, 0.5299,\n",
       "        0.7129, 0.6245, 0.7136, 0.6475, 0.5297, 0.7072, 0.4490, 0.8685, 0.7627,\n",
       "        0.6269, 0.6051, 0.6547, 0.8062, 0.5455, 0.6182, 0.5735, 0.7497, 0.6004,\n",
       "        0.6683, 0.6470, 0.5604, 0.7686, 0.7416, 0.6018, 0.5121, 0.6591, 0.5705,\n",
       "        0.6655, 0.6588, 0.6701, 0.5327, 0.6465, 0.5636, 0.4149, 0.6195, 0.5940,\n",
       "        0.6384, 0.6031, 0.5539, 0.9977, 0.5195, 0.6440, 0.6555, 0.9041, 0.6991,\n",
       "        0.6804, 0.4588, 0.6795, 0.7383, 0.6045, 0.5653, 0.6334, 0.5808, 0.6863,\n",
       "        0.7916, 0.5981, 0.5723, 0.4953, 0.8201, 0.6955, 0.5735, 0.6725, 0.5109,\n",
       "        0.7667, 0.6740, 0.6959, 0.6731, 0.6026, 0.6424, 0.5785, 0.7026, 0.5947,\n",
       "        0.6973, 0.5749, 0.5546, 0.6718, 0.4969, 0.5813, 0.6068, 0.5376, 0.6747,\n",
       "        4.8855, 0.5835, 0.5293, 0.7699, 0.8874, 0.5892, 0.7355, 0.5822, 0.6184,\n",
       "        0.6455, 0.6873, 0.5463, 0.8966, 0.6738, 0.6447, 0.6490, 0.7006, 0.5414,\n",
       "        0.5934, 0.4637, 0.5614, 0.8713, 0.5507, 0.5686, 0.5523, 0.8160, 0.6319,\n",
       "        0.6320, 0.5548, 0.6421, 0.4447, 0.7113, 0.5925, 0.5983, 0.5595, 0.5558,\n",
       "        0.6410, 0.6453, 0.6811, 0.6190, 0.6218, 0.7393, 0.6191, 0.4941, 0.7398,\n",
       "        0.8318, 0.5402, 1.1611, 0.6495, 0.8090, 0.6167, 1.0272, 0.7791, 0.6601,\n",
       "        0.6955, 0.5326, 0.5987, 0.6064, 0.6208, 0.5232, 0.6119, 0.6559, 0.5973,\n",
       "        0.5882, 0.6328, 0.4577, 0.7201, 0.5086, 0.6313, 0.6112, 0.6342, 0.7099,\n",
       "        0.6798, 0.7751, 0.5549, 0.8384, 0.5436, 0.5490, 0.5597, 0.4486, 0.6312,\n",
       "        0.5661, 0.6627, 0.6415, 0.7333, 0.7511, 0.5242, 0.6357, 0.7544, 0.6459,\n",
       "        0.9439, 0.6014, 0.6147, 0.5655, 0.6788, 0.6190, 0.4643, 0.5773, 0.5135,\n",
       "        0.6102, 0.6883, 0.6986, 0.6197, 0.5658, 0.7629, 0.6471, 0.7444, 0.7841,\n",
       "        0.7129, 0.7130, 0.6841, 0.5833, 0.5985, 0.7272, 0.7059, 0.8709, 0.4928,\n",
       "        0.4913, 0.5678, 0.5794, 0.7004, 0.7800, 0.6491, 0.5603, 0.7910, 0.5847,\n",
       "        0.4902, 0.7239, 6.0559, 0.5289, 0.6155, 0.4601, 0.6154, 0.5821, 0.7530,\n",
       "        0.6007, 0.6238, 0.7527, 0.6419, 0.6440, 0.6686, 0.8038, 0.5640, 0.8198,\n",
       "        0.5528, 0.5974, 0.4893, 0.6241, 0.5354, 0.7961, 0.6635, 0.5792, 0.6248,\n",
       "        0.7682, 0.8342, 0.6635, 0.5943, 0.7072, 0.6528, 0.6710, 0.4357, 0.8840,\n",
       "        0.5720, 0.7690, 0.4900, 0.4518, 0.7035, 0.5922, 1.0117, 0.7415, 0.5602,\n",
       "        0.5986, 0.7662, 0.5761, 0.7089, 0.6308, 0.5850, 0.6118, 0.5136, 0.6520,\n",
       "        0.6572, 0.6287, 0.5805, 0.5243, 0.5362, 0.6782, 0.5459, 0.6406, 0.6472,\n",
       "        0.5802, 0.6471, 0.5496, 0.6237, 0.6526, 0.8793, 0.6826, 0.9173, 0.6468,\n",
       "        0.5168, 0.8897, 0.6194, 0.6066, 0.6649, 0.6131, 0.6197, 0.6332, 0.6455,\n",
       "        0.8131, 0.8780, 0.5779, 0.5632, 0.9102, 0.5780, 0.5707, 0.6097, 0.6662,\n",
       "        0.7605, 0.7502, 0.6103, 0.5471, 0.5941, 0.5222, 0.6055, 0.5552, 0.4522,\n",
       "        0.5938, 0.5684, 0.9390, 0.5450, 0.6667, 0.5611, 0.7249, 0.6608, 0.5997,\n",
       "        0.5981, 0.4409, 0.4184, 0.5120, 0.5835, 0.6321, 0.5095, 0.5426, 0.6542,\n",
       "        0.5794, 0.6481, 0.6244, 0.5007, 0.9076, 0.6792, 0.5695, 0.8244, 0.6185,\n",
       "        0.6746, 0.5602, 0.5762, 0.6735, 0.6116, 0.6059, 0.6441, 0.5764, 0.6125,\n",
       "        0.7821, 0.6616, 0.4921, 0.5635, 0.7567, 0.6704, 0.5513, 0.6137, 0.4880,\n",
       "        0.6126, 0.7563, 0.8214, 0.5860, 0.6528, 0.7395, 0.8098, 0.6088, 0.6039,\n",
       "        0.5726, 0.5465, 0.6583, 0.5853, 0.6475, 0.6301, 0.6112, 0.6661, 0.6294,\n",
       "        0.6468, 0.5336, 0.6902, 0.5320, 0.7054, 0.6300, 0.6000, 0.6343, 0.5203,\n",
       "        0.6086, 0.5117, 0.5084, 0.5943, 0.5325, 0.7738, 0.6029, 0.6306, 0.8151,\n",
       "        0.5892, 0.6522, 0.5302, 0.5742, 0.6225, 0.5874, 0.6060, 0.5673, 0.6206,\n",
       "        0.6738, 0.5923, 0.5073, 1.0666, 0.6834, 0.8137, 0.7698, 0.5776, 0.5225,\n",
       "        0.4987, 0.9019, 0.6220, 0.7941, 0.6431, 0.7587, 0.5256, 0.6948, 0.6430,\n",
       "        0.5842, 0.5511, 1.0301, 0.5603, 0.5228, 0.5777, 0.6044, 0.6871, 0.5460,\n",
       "        0.7511, 0.5523, 0.5070, 0.6332, 0.6461, 0.5507, 0.6361, 0.4921, 0.5206,\n",
       "        0.7198, 0.7366, 0.6129, 0.5918, 0.7161, 0.6650, 0.5994, 0.7587, 0.9139,\n",
       "        0.6891, 0.6412, 0.6602, 0.7653, 0.8312, 0.7615, 0.5497, 0.5559, 0.6300,\n",
       "        0.6639, 0.5475, 0.8974, 0.5923, 0.6062, 0.5269, 0.5713, 0.5372, 0.6099,\n",
       "        0.7255, 0.5880, 0.7832, 0.6026, 0.6603, 0.7333, 0.5650, 0.7519, 0.4716,\n",
       "        0.6888, 0.5888, 0.6508, 0.5708, 0.6063, 0.5728, 0.5279, 0.5854, 0.6224,\n",
       "        0.6150, 0.6926, 0.6573, 0.7263, 0.5608, 0.6863, 0.7021, 0.7356, 0.7150,\n",
       "        0.7363, 0.6463, 0.8239, 0.7258, 0.8696, 0.6443, 0.6128, 0.6556, 0.7935,\n",
       "        0.7049, 0.5363, 0.6359, 0.6065, 0.6530, 0.6930, 0.5071, 0.6154, 0.7143,\n",
       "        0.6374, 0.7180, 0.5611, 0.6000, 0.5222, 0.6120, 0.8222, 0.5970, 0.5370,\n",
       "        0.5520, 0.5677, 0.5157, 0.6220, 0.6096, 0.6404, 0.6744, 0.5724, 0.7027,\n",
       "        0.6549, 0.5779, 0.5824, 0.5749, 0.6816, 0.5234, 0.6627, 0.5705, 0.6162,\n",
       "        0.5418, 0.7022, 0.6096, 0.7208, 0.6113, 0.5960, 0.8591, 0.6412, 0.6128,\n",
       "        0.7830, 0.4561, 0.5590, 0.5912, 0.6131, 0.6485, 0.7302, 0.6236, 0.8631,\n",
       "        0.5759, 0.5904, 0.6845, 0.6494, 0.5283, 0.5815, 0.6445, 0.7648, 0.5507,\n",
       "        0.6436, 0.6227, 0.5458, 0.5811, 0.6430, 0.5964, 0.8237, 0.5397, 0.6171,\n",
       "        0.7368, 0.6142, 0.5706, 0.6168, 0.9965, 0.6381, 0.6619, 0.5450, 0.5962,\n",
       "        0.9459, 0.6811, 0.8688, 0.7320, 0.8072, 0.7492, 0.6350, 0.9351, 0.6928,\n",
       "        0.8235, 0.5899, 0.4723, 0.6737, 0.5660, 0.6146, 0.6063, 0.7496, 0.6072,\n",
       "        0.6189, 0.8201, 0.6424, 0.5451, 0.4586, 0.7237, 0.6976, 0.7909, 0.6714,\n",
       "        0.6641, 0.5571, 0.5825, 0.7167, 0.6895, 0.7012, 0.6347, 0.8602, 0.6608,\n",
       "        0.7652, 1.0127, 0.5799, 0.5745, 0.6402, 0.5389, 0.4908, 0.5896, 0.9265,\n",
       "        0.8499, 0.6521, 0.6288, 0.5751, 0.6060, 0.6603, 0.6922, 0.5712, 0.5806,\n",
       "        0.6006, 0.6677, 0.6824, 0.6017, 0.6515, 0.6881, 0.6841, 0.6601, 0.7862,\n",
       "        0.5366, 0.5707, 0.5269, 0.8900, 0.6117, 0.5560, 0.7814, 0.5440, 0.6239,\n",
       "        0.7651, 0.6087, 0.6092, 0.8830, 0.5834, 0.6776, 0.6918, 0.7047, 0.6753,\n",
       "        0.6026, 0.5505, 0.6322, 0.5506, 0.6987, 0.7006, 0.6694, 0.7405, 0.5579,\n",
       "        0.5699, 0.5663, 0.6557])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_features[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.9790, 0.8445, 0.8675,  ..., 0.8474, 0.7660, 0.9136],\n",
       "         [0.9070, 0.9158, 0.8144,  ..., 0.7567, 0.8066, 0.9238],\n",
       "         [1.0075, 0.7432, 0.8451,  ..., 0.7434, 0.8283, 0.9944],\n",
       "         ...,\n",
       "         [1.0425, 0.7874, 0.8396,  ..., 0.7522, 0.8110, 0.9341],\n",
       "         [0.9855, 0.7346, 0.8040,  ..., 0.8014, 0.8319, 1.0146],\n",
       "         [0.8919, 0.9256, 0.7064,  ..., 0.7855, 0.8086, 0.8012]]),\n",
       " tensor([[0.9255, 0.8587, 0.7663,  ..., 0.7322, 0.6925, 0.9016],\n",
       "         [0.8219, 0.8063, 0.7371,  ..., 0.6367, 0.6655, 0.9970],\n",
       "         [0.9221, 0.7261, 0.7866,  ..., 0.6317, 0.6938, 0.9600],\n",
       "         ...,\n",
       "         [0.9414, 0.7305, 0.7569,  ..., 0.6611, 0.6999, 0.9592],\n",
       "         [0.9391, 0.7184, 0.7201,  ..., 0.6618, 0.7316, 1.0245],\n",
       "         [0.7227, 0.7092, 0.7418,  ..., 0.6525, 0.5743, 0.8499]]),\n",
       " tensor([[0.9307, 0.9139, 0.7352,  ..., 0.6902, 0.7027, 0.9382],\n",
       "         [0.8528, 0.8126, 0.7538,  ..., 0.7255, 0.6938, 1.0781],\n",
       "         [0.9087, 0.7793, 0.7588,  ..., 0.6775, 0.6815, 0.9860],\n",
       "         ...,\n",
       "         [0.8806, 0.7967, 0.7321,  ..., 0.6432, 0.7506, 1.0081],\n",
       "         [0.8879, 0.7799, 0.6944,  ..., 0.6254, 0.7667, 1.0188],\n",
       "         [0.7077, 0.7737, 0.6494,  ..., 0.8664, 0.6327, 0.9470]]),\n",
       " tensor([[1.0329, 0.9290, 0.7077,  ..., 0.6331, 0.7151, 0.9114],\n",
       "         [0.9011, 0.7556, 0.7443,  ..., 0.7016, 0.6363, 1.0850],\n",
       "         [0.9520, 0.7546, 0.7462,  ..., 0.6970, 0.6425, 1.0026],\n",
       "         ...,\n",
       "         [0.9403, 0.7865, 0.7216,  ..., 0.5977, 0.7177, 1.0023],\n",
       "         [0.9652, 0.7645, 0.7093,  ..., 0.6021, 0.7169, 1.0428],\n",
       "         [0.7496, 0.7466, 0.6954,  ..., 0.8119, 0.5613, 0.9188]]),\n",
       " tensor([[0.9489, 0.9030, 0.7420,  ..., 0.5994, 0.6787, 0.8562],\n",
       "         [0.8510, 0.7302, 0.7461,  ..., 0.6333, 0.6332, 0.9584],\n",
       "         [0.8776, 0.7400, 0.7207,  ..., 0.6188, 0.6462, 0.9500],\n",
       "         ...,\n",
       "         [0.9494, 0.7962, 0.6940,  ..., 0.5844, 0.6975, 0.9575],\n",
       "         [0.9176, 0.7447, 0.6751,  ..., 0.6412, 0.6454, 0.9657],\n",
       "         [0.7589, 0.8324, 0.7337,  ..., 0.6048, 0.5918, 0.8829]]),\n",
       " tensor([[0.9273, 0.8995, 0.7589,  ..., 0.5811, 0.6681, 0.8465],\n",
       "         [0.8481, 0.7258, 0.7197,  ..., 0.6469, 0.6569, 0.9872],\n",
       "         [0.9131, 0.8245, 0.7403,  ..., 0.6200, 0.6990, 0.9798],\n",
       "         ...,\n",
       "         [0.9184, 0.8173, 0.6677,  ..., 0.6285, 0.6920, 0.9395],\n",
       "         [0.8802, 0.7930, 0.7319,  ..., 0.6766, 0.6966, 1.0383],\n",
       "         [0.7200, 0.9114, 0.6525,  ..., 0.5315, 0.5528, 0.9372]]),\n",
       " tensor([[0.8724, 0.8989, 0.7192,  ..., 0.6358, 0.6185, 0.8418],\n",
       "         [0.7965, 0.7630, 0.7008,  ..., 0.7283, 0.6630, 0.9938],\n",
       "         [0.8964, 0.8333, 0.7576,  ..., 0.6378, 0.7098, 0.9187],\n",
       "         ...,\n",
       "         [0.9045, 0.8258, 0.6763,  ..., 0.6723, 0.6584, 0.9374],\n",
       "         [0.8415, 0.8483, 0.7081,  ..., 0.6791, 0.7152, 1.0156],\n",
       "         [0.7970, 0.8497, 0.6007,  ..., 0.6216, 0.5794, 0.9022]]),\n",
       " tensor([[0.8337, 0.9078, 0.6718,  ..., 0.7161, 0.5428, 0.8476],\n",
       "         [0.7957, 0.8277, 0.7147,  ..., 0.7102, 0.6110, 0.9289],\n",
       "         [0.8761, 0.8555, 0.7034,  ..., 0.6686, 0.6703, 0.8832],\n",
       "         ...,\n",
       "         [0.8994, 0.8910, 0.6733,  ..., 0.6847, 0.6299, 0.8853],\n",
       "         [0.8247, 0.8865, 0.6751,  ..., 0.6853, 0.7327, 1.0367],\n",
       "         [0.8365, 0.8752, 0.5958,  ..., 0.6473, 0.5656, 0.8835]]),\n",
       " tensor([[0.8005, 0.8555, 0.7052,  ..., 0.7291, 0.6098, 0.8147],\n",
       "         [0.7827, 0.7688, 0.7203,  ..., 0.6583, 0.6388, 0.8116],\n",
       "         [0.8925, 0.9089, 0.6604,  ..., 0.6965, 0.7488, 0.8275],\n",
       "         ...,\n",
       "         [0.8657, 0.8881, 0.6664,  ..., 0.6456, 0.6654, 0.7969],\n",
       "         [0.8696, 0.9327, 0.6807,  ..., 0.6840, 0.7148, 0.9441],\n",
       "         [0.8338, 0.8475, 0.6311,  ..., 0.5754, 0.6030, 0.7884]]),\n",
       " tensor([[0.7412, 0.7666, 0.7072,  ..., 0.6826, 0.5875, 0.7275],\n",
       "         [0.7430, 0.7197, 0.7200,  ..., 0.6375, 0.6528, 0.6602],\n",
       "         [0.8805, 0.8554, 0.6092,  ..., 0.6576, 0.7457, 0.7507],\n",
       "         ...,\n",
       "         [0.9125, 0.8591, 0.6807,  ..., 0.5983, 0.6456, 0.7383],\n",
       "         [0.8461, 0.8537, 0.7045,  ..., 0.5786, 0.6198, 0.8049],\n",
       "         [0.7468, 0.6101, 0.5587,  ..., 0.5349, 0.5740, 0.6238]]),\n",
       " tensor([[0.7440, 0.8559, 0.6297,  ..., 0.5790, 0.7823, 0.6938],\n",
       "         [0.7394, 0.7889, 0.6352,  ..., 0.5473, 0.6707, 0.6660],\n",
       "         [0.8090, 0.9588, 0.5472,  ..., 0.5937, 0.7951, 0.6818],\n",
       "         ...,\n",
       "         [0.8935, 0.7717, 0.7522,  ..., 0.5721, 0.6842, 0.7189],\n",
       "         [0.9143, 0.9002, 0.6164,  ..., 0.5511, 0.6558, 0.8157],\n",
       "         [0.6544, 0.5953, 0.5447,  ..., 0.4394, 0.5350, 0.6023]]),\n",
       " tensor([[0.7077, 0.6871, 0.6129,  ..., 0.5699, 0.5663, 0.6557],\n",
       "         [0.6207, 0.6380, 0.4719,  ..., 0.5310, 0.4642, 0.5211],\n",
       "         [0.6930, 0.7959, 0.4703,  ..., 0.4824, 0.5620, 0.6201],\n",
       "         ...,\n",
       "         [0.9113, 0.7065, 0.7061,  ..., 0.5806, 0.6119, 0.6828],\n",
       "         [0.8338, 0.7520, 0.5317,  ..., 0.5699, 0.5317, 0.7167],\n",
       "         [0.4967, 0.4385, 0.4110,  ..., 0.3561, 0.3861, 0.4610]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6726e-05)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def combine_standard_deviation_mean(std_devs, means,token_length):\n",
    "    n = len(std_devs)\n",
    "    if n == 0:\n",
    "        return None, None\n",
    "    mean = torch.mean(means, dim=0)\n",
    "    q = torch.sum((token_length-1)*std_devs + token_length*torch.pow(means, 2),dim=0)\n",
    "    std_dev = torch.sqrt( (q - (token_length*std_devs.shape[0])*torch.pow(mean, 2))/(token_length*std_devs.shape[0]-1) )\n",
    "    return std_dev, mean\n",
    "\n",
    "# sanity check\n",
    "x = torch.rand((100*256,756))\n",
    "std1=torch.std(x,dim=0)\n",
    "\n",
    "x_2 = x.view((100,256,756))\n",
    "std = torch.std(x_2, dim=1)\n",
    "mean = torch.mean(x_2, dim=1)\n",
    "std,_ = combine_standard_deviation_mean(std,mean,token_length)\n",
    "print(torch.std(std1-std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5362, 0.5366, 0.5353, 0.5358, 0.5379, 0.5371, 0.5365, 0.5371, 0.5372,\n",
      "        0.5360, 0.5357, 0.5369, 0.5359, 0.5380, 0.5385, 0.5357, 0.5361, 0.5358,\n",
      "        0.5369, 0.5357, 0.5364, 0.5367, 0.5372, 0.5368, 0.5359, 0.5363, 0.5370,\n",
      "        0.5367, 0.5362, 0.5362, 0.5357, 0.5367, 0.5355, 0.5374, 0.5356, 0.5366,\n",
      "        0.5365, 0.5365, 0.5359, 0.5357, 0.5369, 0.5352, 0.5372, 0.5366, 0.5366,\n",
      "        0.5371, 0.5371, 0.5363, 0.5378, 0.5360, 0.5363, 0.5367, 0.5373, 0.5364,\n",
      "        0.5369, 0.5364, 0.5359, 0.5369, 0.5362, 0.5367, 0.5355, 0.5354, 0.5360,\n",
      "        0.5358, 0.5368, 0.5365, 0.5354, 0.5367, 0.5363, 0.5365, 0.5365, 0.5356,\n",
      "        0.5371, 0.5369, 0.5353, 0.5373, 0.5349, 0.5363, 0.5358, 0.5364, 0.5366,\n",
      "        0.5362, 0.5364, 0.5361, 0.5363, 0.5375, 0.5349, 0.5381, 0.5372, 0.5352,\n",
      "        0.5357, 0.5352, 0.5360, 0.5357, 0.5362, 0.5362, 0.5363, 0.5377, 0.5360,\n",
      "        0.5359, 0.5361, 0.5372, 0.5363, 0.5363, 0.5361, 0.5360, 0.5367, 0.5351,\n",
      "        0.5366, 0.5363, 0.5361, 0.5363, 0.5370, 0.5373, 0.5369, 0.5355, 0.5368,\n",
      "        0.5353, 0.5354, 0.5348, 0.5365, 0.5358, 0.5357, 0.5356, 0.5359, 0.5364,\n",
      "        0.5373, 0.5362, 0.5364, 0.5359, 0.5363, 0.5367, 0.5356, 0.5364, 0.5359,\n",
      "        0.5369, 0.5356, 0.5370, 0.5361, 0.5375, 0.5358, 0.5357, 0.5370, 0.5373,\n",
      "        0.5372, 0.5358, 0.5357, 0.5359, 0.5362, 0.5359, 0.5356, 0.5373, 0.5363,\n",
      "        0.5360, 0.5370, 0.5355, 0.5373, 0.5365, 0.5363, 0.5361, 0.5359, 0.5380,\n",
      "        0.5358, 0.5362, 0.5353, 0.5382, 0.5358, 0.5356, 0.5356, 0.5363, 0.5377,\n",
      "        0.5364, 0.5361, 0.5385, 0.5370, 0.5359, 0.5359, 0.5364, 0.5372, 0.5361,\n",
      "        0.5376, 0.5353, 0.5346, 0.5364, 0.5357, 0.5369, 0.5363, 0.5371, 0.5353,\n",
      "        0.5366, 0.5365, 0.5353, 0.5363, 0.5363, 0.5354, 0.5350, 0.5359, 0.5357,\n",
      "        0.5381, 0.5365, 0.5377, 0.5374, 0.5349, 0.5359, 0.5367, 0.5366, 0.5352,\n",
      "        0.5364, 0.5369, 0.5374, 0.5364, 0.5365, 0.5362, 0.5373, 0.5361, 0.5374,\n",
      "        0.5373, 0.5376, 0.5372, 0.5359, 0.5361, 0.5369, 0.5371, 0.5372, 0.5371,\n",
      "        0.5353, 0.5367, 0.5367, 0.5368, 0.5364, 0.5364, 0.5371, 0.5363, 0.5368,\n",
      "        0.5369, 0.5371, 0.5366, 0.5363, 0.5357, 0.5369, 0.5357, 0.5387, 0.5358,\n",
      "        0.5370, 0.5356, 0.5360, 0.5363, 0.5369, 0.5360, 0.5360, 0.5373, 0.5365,\n",
      "        0.5370, 0.5370, 0.5357, 0.5356, 0.5368, 0.5362, 0.5361, 0.5377, 0.5372,\n",
      "        0.5367, 0.5369, 0.5369, 0.5359, 0.5379, 0.5368, 0.5364, 0.5355, 0.5358,\n",
      "        0.5369, 0.5368, 0.5358, 0.5359, 0.5379, 0.5357, 0.5354, 0.5366, 0.5365,\n",
      "        0.5367, 0.5372, 0.5371, 0.5372, 0.5357, 0.5358, 0.5372, 0.5351, 0.5373,\n",
      "        0.5359, 0.5361, 0.5375, 0.5373, 0.5361, 0.5350, 0.5369, 0.5359, 0.5367,\n",
      "        0.5356, 0.5371, 0.5360, 0.5368, 0.5363, 0.5367, 0.5358, 0.5343, 0.5367,\n",
      "        0.5367, 0.5367, 0.5374, 0.5375, 0.5353, 0.5363, 0.5373, 0.5370, 0.5362,\n",
      "        0.5360, 0.5368, 0.5375, 0.5347, 0.5352, 0.5362, 0.5360, 0.5370, 0.5370,\n",
      "        0.5358, 0.5358, 0.5364, 0.5363, 0.5372, 0.5358, 0.5348, 0.5353, 0.5366,\n",
      "        0.5350, 0.5370, 0.5371, 0.5366, 0.5358, 0.5361, 0.5361, 0.5364, 0.5357,\n",
      "        0.5368, 0.5367, 0.5356, 0.5368, 0.5361, 0.5367, 0.5364, 0.5362, 0.5363,\n",
      "        0.5360, 0.5350, 0.5372, 0.5363, 0.5367, 0.5378, 0.5373, 0.5365, 0.5359,\n",
      "        0.5366, 0.5359, 0.5361, 0.5363, 0.5382, 0.5358, 0.5354, 0.5368, 0.5357,\n",
      "        0.5370, 0.5374, 0.5362, 0.5371, 0.5374, 0.5365, 0.5376, 0.5355, 0.5370,\n",
      "        0.5352, 0.5378, 0.5380, 0.5361, 0.5360, 0.5369, 0.5369, 0.5363, 0.5357,\n",
      "        0.5369, 0.5370, 0.5369, 0.5370, 0.5365, 0.5366, 0.5364, 0.5368, 0.5385,\n",
      "        0.5357, 0.5384, 0.5361, 0.5360, 0.5351, 0.5367, 0.5370, 0.5353, 0.5360,\n",
      "        0.5367, 0.5372, 0.5366, 0.5363, 0.5365, 0.5355, 0.5361, 0.5374, 0.5356,\n",
      "        0.5366, 0.5370, 0.5370, 0.5375, 0.5359, 0.5369, 0.5370, 0.5365, 0.5361,\n",
      "        0.5368, 0.5358, 0.5357, 0.5369, 0.5368, 0.5356, 0.5359, 0.5360, 0.5367,\n",
      "        0.5374, 0.5368, 0.5377, 0.5359, 0.5361, 0.5366, 0.5366, 0.5371, 0.5356,\n",
      "        0.5373, 0.5343, 0.5371, 0.5359, 0.5370, 0.5362, 0.5355, 0.5357, 0.5361,\n",
      "        0.5374, 0.5365, 0.5348, 0.5370, 0.5359, 0.5364, 0.5374, 0.5365, 0.5356,\n",
      "        0.5358, 0.5358, 0.5367, 0.5376, 0.5374, 0.5356, 0.5382, 0.5343, 0.5358,\n",
      "        0.5363, 0.5347, 0.5341, 0.5359, 0.5375, 0.5361, 0.5376, 0.5377, 0.5352,\n",
      "        0.5361, 0.5363, 0.5374, 0.5357, 0.5378, 0.5361, 0.5365, 0.5351, 0.5359,\n",
      "        0.5372, 0.5374, 0.5359, 0.5358, 0.5354, 0.5363, 0.5356, 0.5367, 0.5366,\n",
      "        0.5366, 0.5356, 0.5367, 0.5370, 0.5366, 0.5373, 0.5357, 0.5370, 0.5362,\n",
      "        0.5363, 0.5371, 0.5366, 0.5360, 0.5364, 0.5372, 0.5362, 0.5370, 0.5382,\n",
      "        0.5361, 0.5345, 0.5365, 0.5359, 0.5364, 0.5367, 0.5363, 0.5360, 0.5365,\n",
      "        0.5367, 0.5375, 0.5370, 0.5365, 0.5368, 0.5370, 0.5370, 0.5374, 0.5379,\n",
      "        0.5362, 0.5358, 0.5369, 0.5354, 0.5363, 0.5363, 0.5357, 0.5366, 0.5358,\n",
      "        0.5367, 0.5370, 0.5364, 0.5364, 0.5370, 0.5359, 0.5362, 0.5363, 0.5370,\n",
      "        0.5367, 0.5366, 0.5350, 0.5368, 0.5360, 0.5375, 0.5364, 0.5356, 0.5357,\n",
      "        0.5363, 0.5362, 0.5362, 0.5366, 0.5366, 0.5365, 0.5364, 0.5369, 0.5365,\n",
      "        0.5367, 0.5360, 0.5385, 0.5362, 0.5368, 0.5364, 0.5370, 0.5368, 0.5373,\n",
      "        0.5358, 0.5352, 0.5369, 0.5363, 0.5358, 0.5367, 0.5359, 0.5367, 0.5364,\n",
      "        0.5361, 0.5358, 0.5353, 0.5355, 0.5361, 0.5368, 0.5357, 0.5356, 0.5361,\n",
      "        0.5370, 0.5342, 0.5374, 0.5359, 0.5358, 0.5365, 0.5367, 0.5378, 0.5364,\n",
      "        0.5353, 0.5361, 0.5354, 0.5355, 0.5354, 0.5372, 0.5369, 0.5383, 0.5368,\n",
      "        0.5355, 0.5374, 0.5362, 0.5367, 0.5359, 0.5366, 0.5371, 0.5360, 0.5362,\n",
      "        0.5365, 0.5364, 0.5371, 0.5357, 0.5367, 0.5374, 0.5351, 0.5363, 0.5359,\n",
      "        0.5363, 0.5360, 0.5367, 0.5363, 0.5372, 0.5364, 0.5369, 0.5352, 0.5364,\n",
      "        0.5360, 0.5364, 0.5374, 0.5364, 0.5369, 0.5360, 0.5371, 0.5372, 0.5355,\n",
      "        0.5351, 0.5373, 0.5364, 0.5370, 0.5357, 0.5364, 0.5369, 0.5367, 0.5364,\n",
      "        0.5371, 0.5360, 0.5369, 0.5366, 0.5366, 0.5352, 0.5368, 0.5350, 0.5363,\n",
      "        0.5370, 0.5370, 0.5363, 0.5359, 0.5360, 0.5368, 0.5357, 0.5361, 0.5365,\n",
      "        0.5377, 0.5350, 0.5373, 0.5367, 0.5358, 0.5362, 0.5361, 0.5366, 0.5357,\n",
      "        0.5354, 0.5368, 0.5360, 0.5378, 0.5359, 0.5362, 0.5350, 0.5347, 0.5357,\n",
      "        0.5373, 0.5362, 0.5365, 0.5363, 0.5350, 0.5359, 0.5367, 0.5357, 0.5354,\n",
      "        0.5377, 0.5356, 0.5370, 0.5364, 0.5351, 0.5361, 0.5370, 0.5373, 0.5367,\n",
      "        0.5372, 0.5359, 0.5372, 0.5358, 0.5356, 0.5374, 0.5368, 0.5369, 0.5362,\n",
      "        0.5364, 0.5359, 0.5364, 0.5368, 0.5363, 0.5369, 0.5357, 0.5361, 0.5359,\n",
      "        0.5368, 0.5369, 0.5364, 0.5365, 0.5367, 0.5358, 0.5369, 0.5372, 0.5367,\n",
      "        0.5369, 0.5357, 0.5360, 0.5369, 0.5365, 0.5362, 0.5356, 0.5375, 0.5369,\n",
      "        0.5365, 0.5365, 0.5368, 0.5365, 0.5345, 0.5352, 0.5359, 0.5355, 0.5368])\n"
     ]
    }
   ],
   "source": [
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9641)\n",
      "tensor(0.9008)\n",
      "tensor(0.8825)\n",
      "tensor(0.8830)\n",
      "tensor(0.8708)\n",
      "tensor(0.8718)\n",
      "tensor(0.8671)\n",
      "tensor(0.8688)\n",
      "tensor(0.8759)\n",
      "tensor(0.8661)\n",
      "tensor(0.8725)\n",
      "tensor(0.8726)\n"
     ]
    }
   ],
   "source": [
    "out_list = []\n",
    "for i in range(0,len(std_features)):\n",
    "    std,mean = combine_standard_deviation_mean(std_features[i],mean_features[i],256)\n",
    "    out_list.append(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT_IMDB_STD_FEATURE_DIM = torch.cat(out_list)\n",
    "torch.save(x, 'BERT_IMDB_STD_FEATURE_DIM.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "def combine_standard_deviation(std_devs, means):\n",
    "    n = len(std_devs)\n",
    "    if n == 0 or torch.isnan(std_devs).any() or torch.isnan(means).any():\n",
    "        return None\n",
    "    mean = torch.mean(means)\n",
    "    variance = torch.mean(std_devs ** 2 + means ** 2) - mean ** 2\n",
    "    std_dev = torch.sqrt(variance + 1e-8)\n",
    "    return std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
